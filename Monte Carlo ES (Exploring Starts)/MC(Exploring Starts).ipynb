{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy with gamma = 0:\n",
      "| ⬆️ | ⬆️ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⬆️ | ⬆️ | ❌ | ❌ | ⬆️ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⬆️ | ❌ | ⬆️ | ❌ | ❌ | \n",
      "| ⬆️ | ⬆️ | ⬆️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬆️ | ⬆️ | ❌ | \n",
      "Optimal Policy with gamma = 0.1:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.2:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.5:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ⬆️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.6:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.7:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.8:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ⬅️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.9:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 1:\n",
      "| ⬆️ | ⬆️ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⬆️ | ⬆️ | ❌ | ❌ | ⬆️ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⬆️ | ❌ | ⬆️ | ❌ | ❌ | \n",
      "| ⬆️ | ⬆️ | ⬆️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ❌ | \n",
      "| 🏁 | ⬆️ | ⬆️ | ⬆️ | ❌ | \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the environment\n",
    "grid = np.array([\n",
    "    [ 1,  1,  1,  1,  1],\n",
    "    [ 1,  1, -1, -1,  1],\n",
    "    [-1, -1,  1,  1,  1],\n",
    "    [ 1, -1,  1, -1, -1],\n",
    "    [ 1,  1,  1, -1, -1],\n",
    "    [-1, -1,  1,  1, -1],\n",
    "    [ 2,  1,  1,  1, -1]\n",
    "])\n",
    "\n",
    "# Define the possible actions\n",
    "actions = [0, 1, 2, 3] # up, right, down, left, Terminal\n",
    "action_move = {\n",
    "            0: (-1, 0),\n",
    "            1: (0, 1),\n",
    "            2: (1, 0),\n",
    "            3: (0, -1),\n",
    "        }\n",
    "\n",
    "# Check if the next state is valid\n",
    "def is_valid_state(state):\n",
    "    x, y = state\n",
    "    if x < 0 or x >= grid.shape[0] or y < 0 or y >= grid.shape[1]:\n",
    "        return False\n",
    "    if grid[x, y] == -1:\n",
    "        return False\n",
    "    return True\n",
    "# Get next state\n",
    "\n",
    "def step(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action_move[action]\n",
    "    next_state = (x + dx, y + dy)\n",
    "    if is_valid_state(next_state):\n",
    "        return next_state\n",
    "    return state\n",
    "\n",
    "def calculate_returns(episode, returns, gamma):\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for t in reversed(range(len(episode))):\n",
    "        state, action, reward = episode[t]\n",
    "        G = gamma * G + reward\n",
    "        returns.insert(0, (state, action, G))\n",
    "        # returns[(state, action)].insert(0, G)\n",
    "    return returns\n",
    "\n",
    "def choose_action(epsilon, Q, state):\n",
    "    if state == (6,0):\n",
    "        return 4\n",
    "    r = random.uniform(0,1)\n",
    "    if r < epsilon:\n",
    "        action = random.choice(actions)\n",
    "    else:\n",
    "        action = max(actions, key=lambda x: Q[(state, x)])\n",
    "    return action\n",
    "\n",
    "# Monte Carlo Exploring Starts\n",
    "def monte_carlo_es(grid, gamma, episodes=1000):\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    policy = {}\n",
    "    R_sum = defaultdict(int)\n",
    "    R_count = defaultdict(int)\n",
    "    epsilon = 1\n",
    "\n",
    "    # Initialize state-action pairs\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            if grid[i, j] == 1:\n",
    "                for action in actions:\n",
    "                    Q[((i, j), action)] = 0\n",
    "\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Generate random start state and action\n",
    "        start_state = (random.randint(0, grid.shape[0] - 1), random.randint(0, grid.shape[1] - 1))\n",
    "        while not is_valid_state(start_state):\n",
    "            start_state = (random.randint(0, grid.shape[0] - 1), random.randint(0, grid.shape[1] - 1))\n",
    "        start_action = choose_action(epsilon, Q, start_state)\n",
    "        \n",
    "        # Generate an episode\n",
    "        episode = []\n",
    "        state = start_state\n",
    "        action = start_action\n",
    "        while grid[state[0], state[1]] != 2:\n",
    "            next_state = step(state, action)\n",
    "            reward = 1 if grid[next_state[0], next_state[1]] == 2 else 0\n",
    "            episode.append((state, action, reward))\n",
    "            if grid[next_state[0], next_state[1]] == 2:\n",
    "                episode.append((next_state, 4, 0))\n",
    "                break\n",
    "            state = next_state\n",
    "            action = choose_action(epsilon, Q, state)\n",
    "            \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, returns, gamma)\n",
    "        visited_state_action_pairs = set()\n",
    "        for state, action, ret in returns:\n",
    "            if state not in visited_state_action_pairs:\n",
    "                R_sum[(state, action)] += ret\n",
    "                R_count[(state, action)] += 1     \n",
    "                visited_state_action_pairs.add((state, action))\n",
    "    for (state ,action), ret in R_sum.items():\n",
    "        Q[(state, action)] = ret / R_count[(state, action)]\n",
    "        if state == (6,0):\n",
    "            policy[state] = 4\n",
    "        else:\n",
    "            policy[state] = max(actions, key=lambda x: Q[(state, x)])\n",
    "    return policy\n",
    "\n",
    "\n",
    "emojis = ['⬆️', '➡️', '⬇️', '⬅️', '🏁']\n",
    "Gammas = [0, .1, .2, .5, .6, .7, .8, .9, 1]\n",
    "# Print the optimal policy\n",
    "for gamma in Gammas:\n",
    "    optimal_policy = monte_carlo_es(grid, gamma)\n",
    "    print(f\"Optimal Policy with gamma = {gamma}:\")\n",
    "    for i in range(grid.shape[0]):\n",
    "        print('|', end=' ')\n",
    "        for j in range(grid.shape[1]):\n",
    "            if grid[i, j] == 1 or grid[i, j] == 2:\n",
    "                print(emojis[int(optimal_policy[(i, j)])], end=' | ')\n",
    "            else:\n",
    "                print('❌', end=' | ')  # Using '❌' for obstacles\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy with gamma = 0:\n",
      "| ⬆️ | ⬆️ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⭐ | ⬆️ | ❌ | ❌ | ⬆️ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⬆️ | ❌ | ⬆️ | ❌ | ❌ | \n",
      "| ⬆️ | ⬆️ | ⬆️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬆️ | ⬆️ | ❌ | \n",
      "Optimal Policy with gamma = 0.1:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⭐ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.2:\n",
      "| ⬇️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⭐ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.5:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⭐ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.6:\n",
      "| ⭐ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.7:\n",
      "| ⭐ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ➡️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 0.8:\n",
      "| ➡️ | ➡️ | ➡️ | ➡️ | ⬇️ | \n",
      "| ⬆️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⭐ | ❌ | \n",
      "Optimal Policy with gamma = 0.9:\n",
      "| ➡️ | ➡️ | ➡️ | ⭐ | ⬇️ | \n",
      "| ⬆️ | ⬆️ | ❌ | ❌ | ⬇️ | \n",
      "| ❌ | ❌ | ⬇️ | ⬅️ | ⬅️ | \n",
      "| ⬇️ | ❌ | ⬇️ | ❌ | ❌ | \n",
      "| ➡️ | ➡️ | ⬇️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬇️ | ⬇️ | ❌ | \n",
      "| 🏁 | ⬅️ | ⬅️ | ⬅️ | ❌ | \n",
      "Optimal Policy with gamma = 1:\n",
      "| ⬆️ | ⬆️ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⬆️ | ⬆️ | ❌ | ❌ | ⬆️ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ⬆️ | \n",
      "| ⬆️ | ❌ | ⭐ | ❌ | ❌ | \n",
      "| ⬆️ | ⬆️ | ⬆️ | ❌ | ❌ | \n",
      "| ❌ | ❌ | ⬆️ | ⬆️ | ❌ | \n",
      "| 🏁 | ⬆️ | ⬆️ | ⬆️ | ❌ | \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the environment\n",
    "grid = np.array([\n",
    "    [ 1,  1,  1,  1,  1],\n",
    "    [ 1,  1, -1, -1,  1],\n",
    "    [-1, -1,  1,  1,  1],\n",
    "    [ 1, -1,  1, -1, -1],\n",
    "    [ 1,  1,  1, -1, -1],\n",
    "    [-1, -1,  1,  1, -1],\n",
    "    [ 2,  1,  1,  1, -1]\n",
    "])\n",
    "\n",
    "# Define the possible actions\n",
    "actions = [0, 1, 2, 3] # up, right, down, left, Terminal\n",
    "action_move = {\n",
    "    0: (-1, 0),\n",
    "    1: (0, 1),\n",
    "    2: (1, 0),\n",
    "    3: (0, -1),\n",
    "}\n",
    "\n",
    "# Check if the next state is valid\n",
    "def is_valid_state(state):\n",
    "    x, y = state\n",
    "    if x < 0 or x >= grid.shape[0] or y < 0 or y >= grid.shape[1]:\n",
    "        return False\n",
    "    if grid[x, y] == -1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Get next state\n",
    "def step(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action_move[action]\n",
    "    next_state = (x + dx, y + dy)\n",
    "    if is_valid_state(next_state):\n",
    "        return next_state\n",
    "    return state\n",
    "\n",
    "def calculate_returns(episode, gamma):\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for t in reversed(range(len(episode))):\n",
    "        state, action, reward = episode[t]\n",
    "        G = gamma * G + reward\n",
    "        returns.insert(0, (state, action, G))\n",
    "    return returns\n",
    "\n",
    "def choose_action(epsilon, Q, state):\n",
    "    if state == (6,0):\n",
    "        return 4\n",
    "    r = random.uniform(0,1)\n",
    "    if r < epsilon:\n",
    "        action = random.choice(actions)\n",
    "    else:\n",
    "        action = max(actions, key=lambda x: Q[(state, x)])\n",
    "    return action\n",
    "\n",
    "# Monte Carlo Exploring Starts\n",
    "def monte_carlo_es(grid, gamma, episodes=1000):\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    policy = {}\n",
    "    R_sum = defaultdict(int)\n",
    "    R_count = defaultdict(int)\n",
    "    epsilon = 1\n",
    "\n",
    "    # Initialize state-action pairs\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            if grid[i, j] == 1:\n",
    "                for action in actions:\n",
    "                    Q[((i, j), action)] = 0\n",
    "\n",
    "    final_start_state = None  # Track the starting state of the final episode\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # Generate random start state and action\n",
    "        start_state = (random.randint(0, grid.shape[0] - 1), random.randint(0, grid.shape[1] - 1))\n",
    "        while not is_valid_state(start_state):\n",
    "            start_state = (random.randint(0, grid.shape[0] - 1), random.randint(0, grid.shape[1] - 1))\n",
    "        final_start_state = start_state  # Update final start state for the current episode\n",
    "        start_action = choose_action(epsilon, Q, start_state)\n",
    "        \n",
    "        # Generate an episode\n",
    "        episode = []\n",
    "        state = start_state\n",
    "        action = start_action\n",
    "        while grid[state[0], state[1]] != 2:\n",
    "            next_state = step(state, action)\n",
    "            reward = 1 if grid[next_state[0], next_state[1]] == 2 else 0\n",
    "            episode.append((state, action, reward))\n",
    "            if grid[next_state[0], next_state[1]] == 2:\n",
    "                episode.append((next_state, 4, 0))\n",
    "                break\n",
    "            state = next_state\n",
    "            action = choose_action(epsilon, Q, state)\n",
    "            \n",
    "        # Calculate returns\n",
    "        returns = calculate_returns(episode, gamma)\n",
    "        visited_state_action_pairs = set()\n",
    "        for state, action, ret in returns:\n",
    "            if state not in visited_state_action_pairs:\n",
    "                R_sum[(state, action)] += ret\n",
    "                R_count[(state, action)] += 1     \n",
    "                visited_state_action_pairs.add((state, action))\n",
    "    \n",
    "    for (state, action), ret in R_sum.items():\n",
    "        Q[(state, action)] = ret / R_count[(state, action)]\n",
    "        if state == (6,0):\n",
    "            policy[state] = 4\n",
    "        else:\n",
    "            policy[state] = max(actions, key=lambda x: Q[(state, x)])\n",
    "    \n",
    "    return policy, final_start_state\n",
    "\n",
    "emojis = ['⬆️', '➡️', '⬇️', '⬅️', '🏁']\n",
    "start_emoji = '⭐'\n",
    "Gammas = [0, .1, .2, .5, .6, .7, .8, .9, 1]\n",
    "\n",
    "# Print the optimal policy\n",
    "for gamma in Gammas:\n",
    "    optimal_policy, start_state = monte_carlo_es(grid, gamma)\n",
    "    print(f\"Optimal Policy with gamma = {gamma}:\")\n",
    "    for i in range(grid.shape[0]):\n",
    "        print('|', end=' ')\n",
    "        for j in range(grid.shape[1]):\n",
    "            if (i, j) == start_state:\n",
    "                print(start_emoji, end=' | ')\n",
    "            elif grid[i, j] == 1 or grid[i, j] == 2:\n",
    "                print(emojis[int(optimal_policy[(i, j)])], end=' | ')\n",
    "            else:\n",
    "                print('❌', end=' | ')  # Using '❌' for obstacles\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
